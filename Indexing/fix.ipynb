{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each SELECTED result, create snippet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet should include:\n",
    "1. document title\n",
    "2. 2 sentences from doc with highest COSINE SIMILARITY w/respect to q. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.inverted_index import inverted_index\n",
    "from classes.persist_index_memory import persist_index_memory\n",
    "from classes.utilities import doc_utilities\n",
    "from classes.preprocessing import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "12700it [00:34, 365.96it/s]\n"
     ]
    }
   ],
   "source": [
    "u = doc_utilities()\n",
    "u.read_data_set(file='data/12000_docs.p')\n",
    "memory_unit = persist_index_memory()\n",
    "u.process_documents_for_indexing()\n",
    "i_i = inverted_index(memory_unit)\n",
    "i_i.create_index(collection=u.get_collection_json(),\n",
    "                     process_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. create index from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index size = 94679\n"
     ]
    }
   ],
   "source": [
    "print(\"Index size = {}\".format(i_i.get_index_size()))\n",
    "# This should be roughly 100k tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. given query, find relevant results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_matches(CR={}, len_DC=1699999):\n",
    "    doc_first = {}\n",
    "    \n",
    "    # better way of generating pertinent indices?\n",
    "    for i in range(1, len_DC+1):\n",
    "        doc_first[i] = []    \n",
    "\n",
    "    for token in CR:\n",
    "        for val in CR[token]:\n",
    "            match = ('match: ' + str(token), val.frequency)\n",
    "            doc_first[val.docId].append(match)\n",
    "    return {k: doc_first[k] for k in doc_first if len(doc_first[k]) > 0}\n",
    "\n",
    "def match_scaling(CR = {}, match_num = int):\n",
    "    return {k: CR[k] for k in CR if len(CR[k]) == match_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"test rEd!\"\n",
    "# p = preprocessing()\n",
    "# text = p.remove_punctuation(text=query)\n",
    "# tokens = p.tokenize(text=text)\n",
    "# tokens = p.remove_stopwords(tokens=tokens)\n",
    "# tokens = p.remove_capitalization(tokens=tokens)\n",
    "# q = p.stem(tokens=tokens)\n",
    "# q = ' '.join(q)\n",
    "# print('Query pre processed = {}'.format(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = i_i.lookup_query(q)\n",
    "CR = return_matches(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select resources that match every token from query\n",
    "CR = match_scaling(CR, len(q.split()))\n",
    "CR = match_scaling(CR, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. rank relevant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from classes.ranking import ranking\n",
    "\n",
    "r_ranking = ranking()\n",
    "resources = list(CR.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'red']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_query_tokens = q.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is necessary for idf\n",
    "i_i.create_term_document_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 53382.05it/s]\n"
     ]
    }
   ],
   "source": [
    "max_freq = r_ranking.get_max_frequencies(index=CR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_i.storage.max_frequency_terms_per_doc = max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 1, 4, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_i.storage.max_frequency_terms_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3918, 5128, 6680, 6882, 8081, 10650, 11014]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'state nation disput local employ the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = r_ranking.relevance_ranking(query = query,\n",
    "                           num_results=5,\n",
    "                            index=i_i.index,\n",
    "                            resources=resources,\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. generate snippets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate cosine similarity between every 'sentence' in doc and q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_similarity(d_weights, q_weights):\n",
    "    numerator = sum([d*q for d,q in zip(d_weights, q_weights)])\n",
    "    d_norm = sum([d*d for d in d_weights])\n",
    "    q_norm = sum([q*q for q in q_weights])\n",
    "    denom = np.sqrt(d_norm * q_norm)\n",
    "    return numerator/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "weigher = ranking()\n",
    "p = preprocessing()\n",
    "def gen_snip(document=[], i_i={}, query=''):\n",
    "    q_weights = weigher.relevance_ranking(query = query,\n",
    "                           num_results=len(sentences[0]),\n",
    "                            index=i_i.index,\n",
    "                            resources=[],\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all,\n",
    "                            weigh=True)\n",
    "    \n",
    "    q_w=ordered_q_weights = [w[1] for w in sorted(q_weights, key=lambda x: x[0])]\n",
    "    \n",
    "    cos_score = -1.0\n",
    "    snippet = collections.deque(maxlen=2)\n",
    "\n",
    "    for i in range(len(document)):\n",
    "        s = document[i]\n",
    "        s_max_freqs = weigher.get_max_frequencies(index=i_i.index, sentence_tokens=p.the_works(text=s))\n",
    "        i_i.storage.max_frequency_terms_per_doc = s_max_freqs\n",
    "\n",
    "        s_weights = weigher.relevance_ranking(query = s,\n",
    "                               num_results=len(s),\n",
    "                                index=i_i.index,\n",
    "                                resources=[],\n",
    "                                max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                                N=len(i_i.storage.index),\n",
    "                                term_doc_matrix=i_i.doc_term_matrix_all,\n",
    "                                weigh=True)\n",
    "\n",
    "        ordered_s_weights = [w[1] for w in sorted(s_weights, key=lambda x: x[0])]\n",
    "\n",
    "        cosine_sim = cos_similarity(ordered_s_weights, q_w)\n",
    "\n",
    "        if cosine_sim > cos_score:\n",
    "            cos_score = cosine_sim\n",
    "            snippet.append(s)\n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3918, 5128, 6680, 6882, 8081, 10650, 11014],\n",
       " 'state nation disput local employ the')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "# id corresponds to doc's index in \"resources\"\n",
    "\n",
    "def generate_snippets(ranked_res, resources, query, i_i):\n",
    "    sent_split = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    json_frontend = []\n",
    "    for id, weight in tqdm(res):\n",
    "        doc_id = resources[id]\n",
    "\n",
    "        # retrieve original unprocessed doc-string    \n",
    "        original = i_i.storage.get(doc_id)['content']\n",
    "\n",
    "        # get doc title\n",
    "        title, *text = original.split('\\n\\n')\n",
    "        text = ' '.join(text)\n",
    "\n",
    "        # separate text into sentences\n",
    "        sentences = sent_split.tokenize(text)\n",
    "\n",
    "        # generate snippet\n",
    "        doc_snippet = gen_snippets(document=sentences, i_i=i_i, query=query)\n",
    "    #     print(doc_snippet)\n",
    "\n",
    "        # prepare for frontend\n",
    "        json_frontend.append({'title':title, 'snippet':'...'.join(doc_snippet)})\n",
    "    \n",
    "    return json_frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/home/david/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "\n",
      " 20%|██        | 1/5 [00:04<00:16,  4.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n",
      "Term themselv not found in index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 2/5 [00:15<00:18,  6.33s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:21<00:12,  6.18s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:31<00:07,  7.40s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:34<00:00,  5.96s/it]\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Marianne Plehn',\n",
       "  'snippet': 'But the field had changed so much in 20 years that Plehn took a different approach....The illustrations and photographs continued to be used for decades.'},\n",
       " {'title': 'List of Wallace and Gromit characters',\n",
       "  'snippet': 'A very homely sort who doesn\\'t mind the odd adventure.\"...Some of Wallace\\'s contraptions are based on real-life inventions.'},\n",
       " {'title': 'Claudia UmpiÃ©rrez',\n",
       "  'snippet': 'Claudia InÃ©s UmpiÃ©rrez RodrÃ\\xadguez (born 6 January 1983) is a Uruguayan association football referee and lawyer by profession....Claudia was enthusiastic and wanted to register, but as a minor she could not.'},\n",
       " {'title': 'Meltdown (security vulnerability)',\n",
       "  'snippet': 'It allows a rogue process to read all memory, even when it is not authorized to do so.'},\n",
       " {'title': 'Harold W. Parsons',\n",
       "  'snippet': \"It was a bargain, and later sold for much more than it cost....Their letter are now at Cambridge University, King's College Archive Centre.\"}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_snippets(res, resources, query, i_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Marianne Plehn',\n",
       "  'snippet': 'But the field had changed so much in 20 years that Plehn took a different approach....The illustrations and photographs continued to be used for decades.'},\n",
       " {'title': 'List of Wallace and Gromit characters',\n",
       "  'snippet': 'A very homely sort who doesn\\'t mind the odd adventure.\"...Some of Wallace\\'s contraptions are based on real-life inventions.'},\n",
       " {'title': 'Claudia UmpiÃ©rrez',\n",
       "  'snippet': 'Claudia InÃ©s UmpiÃ©rrez RodrÃ\\xadguez (born 6 January 1983) is a Uruguayan association football referee and lawyer by profession....Claudia was enthusiastic and wanted to register, but as a minor she could not.'},\n",
       " {'title': 'Meltdown (security vulnerability)',\n",
       "  'snippet': 'It allows a rogue process to read all memory, even when it is not authorized to do so.'},\n",
       " {'title': 'Harold W. Parsons',\n",
       "  'snippet': \"It was a bargain, and later sold for much more than it cost....Their letter are now at Cambridge University, King's College Archive Centre.\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(what does this number mean? Are we looking for the greatest value?)**\n",
    "* -1: exactly opposite; \n",
    "* +1: identical\n",
    "* 0: orthogonal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

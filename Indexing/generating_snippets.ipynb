{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each SELECTED result, create snippet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet should include:\n",
    "1. document title\n",
    "2. 2 sentences from doc with highest COSINE SIMILARITY w/respect to q. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.inverted_index import inverted_index\n",
    "from classes.persist_index_memory import persist_index_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "persist_index_memory.py INFO: 2019-10-07 15:40:54,681: Instantiating storage object\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "inverted_index.py INFO: 2019-10-07 15:40:54,682: Instantiating inverted index object\n"
     ]
    }
   ],
   "source": [
    "storage = persist_index_memory()\n",
    "i_i = inverted_index(storage=storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load original dataset, the doc collection \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "prefix = \"~/Documents/Classes/CS537_IR/project1/chunks/\"\n",
    "sample_path = prefix + 'chunk_17.pkl' \n",
    "doc_collection = pd.read_pickle(sample_path)\n",
    "DC = doc_collection.to_dict('records')\n",
    "# use pandas read_csv(skiprows=FUNCTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. create index from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load inverted index\n",
    "# path = \"../../../Documents/Classes/CS537_IR/project1/\"\n",
    "\n",
    "import pickle\n",
    "from classes.inverted_index import inverted_index, Appearance\n",
    "\n",
    "with open('inverted_index_chunk17.pkl', 'rb') as handle:\n",
    "    i_i = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_i.create_index(DC, process_text=True)\n",
    "\n",
    "# import pickle\n",
    "# with open('inverted_index_chunk17.pkl', 'wb') as handle:\n",
    "#     pickle.dump(i_i, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. given query, find relevant results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_matches(CR={}, len_DC=1699999):\n",
    "    doc_first = {}\n",
    "    \n",
    "    # better way of generating pertinent indices?\n",
    "    for i in range(1, len_DC+1):\n",
    "        doc_first[i] = []    \n",
    "\n",
    "    for token in CR:\n",
    "        for val in CR[token]:\n",
    "            match = ('match: ' + str(token), val.frequency)\n",
    "            doc_first[val.docId].append(match)\n",
    "    return {k: doc_first[k] for k in doc_first if len(doc_first[k]) > 0}\n",
    "\n",
    "def match_scaling(CR = {}, match_num = int):\n",
    "    return {k: CR[k] for k in CR if len(CR[k]) == match_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what time is shark week\"\n",
    "q = preprocess(query)\n",
    "q = ' '.join(q)\n",
    "\n",
    "CR = i_i.lookup_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = return_matches(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select resources that match every token from query\n",
    "CR = match_scaling(CR, len(q.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1602897: [('match: time', 3), ('match: shark', 1), ('match: week', 15)],\n",
       " 1618493: [('match: time', 1), ('match: shark', 1), ('match: week', 1)],\n",
       " 1619433: [('match: time', 2), ('match: shark', 1), ('match: week', 1)],\n",
       " 1628933: [('match: time', 3), ('match: shark', 1), ('match: week', 1)],\n",
       " 1638449: [('match: time', 4), ('match: shark', 1), ('match: week', 3)],\n",
       " 1648033: [('match: time', 2), ('match: shark', 1), ('match: week', 2)],\n",
       " 1649133: [('match: time', 1), ('match: shark', 1), ('match: week', 2)],\n",
       " 1651158: [('match: time', 3), ('match: shark', 1), ('match: week', 1)],\n",
       " 1652709: [('match: time', 5), ('match: shark', 1), ('match: week', 1)],\n",
       " 1661061: [('match: time', 3), ('match: shark', 7), ('match: week', 1)],\n",
       " 1662393: [('match: time', 11), ('match: shark', 3), ('match: week', 2)]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. rank relevant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.ranking import ranking\n",
    "\n",
    "r_ranking = ranking()\n",
    "resources = list(CR.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1602897: [('match: time', 3), ('match: shark', 1), ('match: week', 15)],\n",
       " 1618493: [('match: time', 1), ('match: shark', 1), ('match: week', 1)],\n",
       " 1619433: [('match: time', 2), ('match: shark', 1), ('match: week', 1)],\n",
       " 1628933: [('match: time', 3), ('match: shark', 1), ('match: week', 1)],\n",
       " 1638449: [('match: time', 4), ('match: shark', 1), ('match: week', 3)],\n",
       " 1648033: [('match: time', 2), ('match: shark', 1), ('match: week', 2)],\n",
       " 1649133: [('match: time', 1), ('match: shark', 1), ('match: week', 2)],\n",
       " 1651158: [('match: time', 3), ('match: shark', 1), ('match: week', 1)],\n",
       " 1652709: [('match: time', 5), ('match: shark', 1), ('match: week', 1)],\n",
       " 1661061: [('match: time', 3), ('match: shark', 7), ('match: week', 1)],\n",
       " 1662393: [('match: time', 11), ('match: shark', 3), ('match: week', 2)]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match: time\n",
      "3\n",
      "match: week\n",
      "15\n",
      "match: time\n",
      "1\n",
      "match: time\n",
      "2\n",
      "match: time\n",
      "3\n",
      "match: time\n",
      "4\n",
      "match: time\n",
      "2\n",
      "match: time\n",
      "1\n",
      "match: week\n",
      "2\n",
      "match: time\n",
      "3\n",
      "match: time\n",
      "5\n",
      "match: time\n",
      "3\n",
      "match: shark\n",
      "7\n",
      "match: time\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for docId, matches in CR.items():\n",
    "    tmp_freq = 0\n",
    "    for token in matches:\n",
    "\n",
    "        frequency = token[1]\n",
    "        if frequency > tmp_freq:\n",
    "            print(token[0])\n",
    "            tmp_freq = frequency\n",
    "            print(tmp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'docId'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-df8108bcc839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#i_i.create_term_document_matrix()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GIT/IR-BSU/Indexing/classes/ranking.py\u001b[0m in \u001b[0;36mget_max_frequencies\u001b[0;34m(self, index, num_docs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mmax_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mmax_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_highest_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/IR-BSU/Indexing/classes/ranking.py\u001b[0m in \u001b[0;36mget_highest_frequency\u001b[0;34m(self, index, doc_id)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# of the documents that key(term) occurs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequency\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtmp_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mtmp_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'docId'"
     ]
    }
   ],
   "source": [
    "#i_i.create_term_document_matrix()\n",
    "max_freq = r_ranking.get_max_frequencies(index=CR, num_docs=len(resources)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_i.storage.max_frequency_terms_per_doc = max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = r_ranking.relevance_ranking(query = 'state nation disput local employ the',\n",
    "                           num_results=5,\n",
    "                            index=i_i.index,\n",
    "                            resources=resources,\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. generate snippets from ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate cosine similarity between every 'sentence' in doc and q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve original, unprocessed doc-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = dc[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get doc title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cool python trick:\n",
    "https://stackoverflow.com/questions/31426095/assign-multiple-values-of-a-list\n",
    "'''\n",
    "\n",
    "title, *text = original.split('\\n\\n')\n",
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "sent_split = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = sent_split.tokenize(text)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process sentences same way as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stemer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def stem(tokens=''):\n",
    "    return [stemer.stem(w) for w in tokens]\n",
    "def tokenize(text=''):\n",
    "    return tokenizer.tokenize(text)\n",
    "def remove_stopwords(tokens=[]):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "def remove_capitalization(tokens=[]):\n",
    "    return [w.lower() for w in tokens]\n",
    "def remove_punctuation(text=''):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "def preprocess(text=''):\n",
    "    # Remove punctuation from the text.\n",
    "    text = remove_punctuation(text=text)\n",
    "    # Tokenize\n",
    "    tokens = tokenize(text=text)\n",
    "    # Remove stop words\n",
    "    tokens = remove_stopwords(tokens=tokens)\n",
    "    # Remove capitalization\n",
    "    tokens = remove_capitalization(tokens=tokens)\n",
    "    # Stem terms\n",
    "    tokens = stem(tokens=tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = [preprocess(sentences[i]) for i in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cosine similarity between sentences & the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_weights = []\n",
    "term_query_weights = []\n",
    "\n",
    "# let the query be...\n",
    "query = \"post office hours\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve weights for terms in sentence, + in query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokens[0] \n",
    "for w in sent:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = i_i\n",
    "query = \"what time is shark week\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params: self, query='', num_results=5, index=[], resources=[]\n",
    "'''\n",
    "Calculate relevance ranking\n",
    "query: String containing the query, this is a single string and it is not tokenized\n",
    "resources: Array of document id's that will be processed\n",
    "return: Array of ranked documents.\n",
    "'''\n",
    "\n",
    "ranker.relevance_ranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ranker.relevance_ranking(query = q,\n",
    "                           num_results=5,\n",
    "                            index=i_i.index,\n",
    "                            resources=resources,\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

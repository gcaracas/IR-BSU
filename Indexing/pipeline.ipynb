{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each SELECTED result, create snippet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet should include:\n",
    "1. document title\n",
    "2. 2 sentences from doc with highest COSINE SIMILARITY w/respect to q. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.inverted_index import inverted_index\n",
    "from classes.persist_index_memory import persist_index_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "persist_index_memory.py INFO: 2019-10-07 16:52:02,987: Instantiating storage object\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "inverted_index.py INFO: 2019-10-07 16:52:03,255: Instantiating inverted index object\n"
     ]
    }
   ],
   "source": [
    "storage = persist_index_memory()\n",
    "i_i = inverted_index(storage=storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load original dataset, the doc collection \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "prefix = \"~/Documents/Classes/CS537_IR/project1/chunks/\"\n",
    "sample_path = prefix + 'chunk_17.pkl' \n",
    "doc_collection = pd.read_pickle(sample_path)\n",
    "DC = doc_collection.to_dict('records')\n",
    "# use pandas read_csv(skiprows=FUNCTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. create index from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load inverted index\n",
    "# path = \"../../../Documents/Classes/CS537_IR/project1/\"\n",
    "\n",
    "import pickle\n",
    "from classes.inverted_index import inverted_index, Appearance\n",
    "\n",
    "with open('inverted_index_chunk17.pkl', 'rb') as handle:\n",
    "    i_i = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_i.create_index(DC, process_text=True)\n",
    "\n",
    "# import pickle\n",
    "# with open('inverted_index_chunk17.pkl', 'wb') as handle:\n",
    "#     pickle.dump(i_i, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. given query, find relevant results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_matches(CR={}, len_DC=1699999):\n",
    "    doc_first = {}\n",
    "    \n",
    "    # better way of generating pertinent indices?\n",
    "    for i in range(1, len_DC+1):\n",
    "        doc_first[i] = []    \n",
    "\n",
    "    for token in CR:\n",
    "        for val in CR[token]:\n",
    "            match = ('match: ' + str(token), val.frequency)\n",
    "            doc_first[val.docId].append(match)\n",
    "    return {k: doc_first[k] for k in doc_first if len(doc_first[k]) > 0}\n",
    "\n",
    "def match_scaling(CR = {}, match_num = int):\n",
    "    return {k: CR[k] for k in CR if len(CR[k]) == match_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stemer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def stem(tokens=''):\n",
    "    return [stemer.stem(w) for w in tokens]\n",
    "def tokenize(text=''):\n",
    "    return tokenizer.tokenize(text)\n",
    "def remove_stopwords(tokens=[]):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "def remove_capitalization(tokens=[]):\n",
    "    return [w.lower() for w in tokens]\n",
    "def remove_punctuation(text=''):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "def preprocess(text=''):\n",
    "    # Remove punctuation from the text.\n",
    "    text = remove_punctuation(text=text)\n",
    "    # Tokenize\n",
    "    tokens = tokenize(text=text)\n",
    "    # Remove stop words\n",
    "    tokens = remove_stopwords(tokens=tokens)\n",
    "    # Remove capitalization\n",
    "    tokens = remove_capitalization(tokens=tokens)\n",
    "    # Stem terms\n",
    "    tokens = stem(tokens=tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what time is shark week\"\n",
    "q = preprocess(query)\n",
    "q = ' '.join(q)\n",
    "\n",
    "CR = i_i.lookup_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = return_matches(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select resources that match every token from query\n",
    "CR = match_scaling(CR, len(q.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1602897: [('match: time', 3), ('match: shark', 1), ('match: week', 15)],\n",
       " 1618493: [('match: time', 1), ('match: shark', 1), ('match: week', 1)],\n",
       " 1619433: [('match: time', 2), ('match: shark', 1), ('match: week', 1)],\n",
       " 1628933: [('match: time', 3), ('match: shark', 1), ('match: week', 1)],\n",
       " 1638449: [('match: time', 4), ('match: shark', 1), ('match: week', 3)],\n",
       " 1648033: [('match: time', 2), ('match: shark', 1), ('match: week', 2)],\n",
       " 1649133: [('match: time', 1), ('match: shark', 1), ('match: week', 2)],\n",
       " 1651158: [('match: time', 3), ('match: shark', 1), ('match: week', 1)],\n",
       " 1652709: [('match: time', 5), ('match: shark', 1), ('match: week', 1)],\n",
       " 1661061: [('match: time', 3), ('match: shark', 7), ('match: week', 1)],\n",
       " 1662393: [('match: time', 11), ('match: shark', 3), ('match: week', 2)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. rank relevant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.ranking import ranking\n",
    "\n",
    "r_ranking = ranking()\n",
    "resources = list(CR.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_freq=[]\n",
    "# for docId, matches in CR.items():\n",
    "#     tmp_freq = 0\n",
    "#     for token in matches:\n",
    "\n",
    "#         frequency = token[1]\n",
    "#         if frequency > tmp_freq:\n",
    "# #             print(token[0])\n",
    "#             tmp_freq = frequency\n",
    "# #             print(tmp_freq)\n",
    "#     max_freq.append(tmp_freq)\n",
    "# max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1602897, [('match: time', 3), ('match: shark', 1), ('match: week', 15)]), (1618493, [('match: time', 1), ('match: shark', 1), ('match: week', 1)]), (1619433, [('match: time', 2), ('match: shark', 1), ('match: week', 1)]), (1628933, [('match: time', 3), ('match: shark', 1), ('match: week', 1)]), (1638449, [('match: time', 4), ('match: shark', 1), ('match: week', 3)]), (1648033, [('match: time', 2), ('match: shark', 1), ('match: week', 2)]), (1649133, [('match: time', 1), ('match: shark', 1), ('match: week', 2)]), (1651158, [('match: time', 3), ('match: shark', 1), ('match: week', 1)]), (1652709, [('match: time', 5), ('match: shark', 1), ('match: week', 1)]), (1661061, [('match: time', 3), ('match: shark', 7), ('match: week', 1)]), (1662393, [('match: time', 11), ('match: shark', 3), ('match: week', 2)])])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CR.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'shark', 'week']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_query_tokens = q.split()\n",
    "processed_query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the most frequent use of a term in the collection\n",
    "terms = list(self.index.keys())\n",
    "stemmed_tokens=[]\n",
    "for id, document in relevant_results.items():\n",
    "    text = self.preprocessing.remove_punctuation(text=document['content'])\n",
    "    tokens = self.preprocessing.tokenize(text=text)\n",
    "    tokens = self.preprocessing.remove_stopwords(tokens=tokens)\n",
    "    tokens = self.preprocessing.remove_capitalization(tokens=tokens)\n",
    "    stems = self.preprocessing.stem(tokens=tokens)\n",
    "    for stem in stems:\n",
    "        found = False\n",
    "        for token in tokens:\n",
    "            if token == stem:\n",
    "                found = True\n",
    "        if found == False:\n",
    "            stemmed_tokens.append(stem)\n",
    "# We have now all the stemmed words that didn't match documents, now\n",
    "# let's make a unique list of them.\n",
    "stem_tokens = list(set(stemmed_tokens))\n",
    "\n",
    "# Now let's generate the matrix\n",
    "self.doc_term_matrix_all=[]\n",
    "for id, document in self.storage.index.items():\n",
    "    term_doc_matrix=dict()\n",
    "    text = self.preprocessing.remove_punctuation(text=document['text'])\n",
    "    tokens = self.preprocessing.tokenize(text=text)\n",
    "    tokens = self.preprocessing.remove_stopwords(tokens=tokens)\n",
    "    tokens = self.preprocessing.remove_capitalization(tokens=tokens)\n",
    "    stems = self.preprocessing.stem(tokens=tokens)\n",
    "    for full_stem in stem_tokens:\n",
    "        found = False\n",
    "        for doc_stemmed_token in stems:\n",
    "            if full_stem == doc_stemmed_token:\n",
    "                found = True\n",
    "        if found == True:\n",
    "            term_doc_matrix[full_stem] = 1\n",
    "        else:\n",
    "            term_doc_matrix[full_stem] = 0\n",
    "    self.doc_term_matrix_all.append(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have now all the stemmed words that didn't match documents, now\n",
    "# let's make a unique list of them.\n",
    "'''\n",
    "^???\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-49f1607111c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this step is necessary for idf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mi_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_term_document_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GIT/IR-BSU/Indexing/classes/inverted_index.py\u001b[0m in \u001b[0;36mcreate_term_document_matrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this step is necessary for idf\n",
    "i_i.create_term_document_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 73350.31it/s]\n"
     ]
    }
   ],
   "source": [
    "max_freq = r_ranking.get_max_frequencies(index=CR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_i.storage.max_frequency_terms_per_doc = max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'inverted_index' object has no attribute 'doc_term_matrix_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e3810a3cef8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mmax_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_frequency_terms_per_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                             term_doc_matrix=i_i.doc_term_matrix_all)\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'inverted_index' object has no attribute 'doc_term_matrix_all'"
     ]
    }
   ],
   "source": [
    "res = r_ranking.relevance_ranking(query = 'state nation disput local employ the',\n",
    "                           num_results=5,\n",
    "                            index=i_i.index,\n",
    "                            resources=resources,\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate cosine similarity between every 'sentence' in doc and q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve original, unprocessed doc-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = dc[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get doc title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cool python trick:\n",
    "https://stackoverflow.com/questions/31426095/assign-multiple-values-of-a-list\n",
    "'''\n",
    "\n",
    "title, *text = original.split('\\n\\n')\n",
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "sent_split = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = sent_split.tokenize(text)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process sentences same way as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = [preprocess(sentences[i]) for i in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cosine similarity between sentences & the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_weights = []\n",
    "term_query_weights = []\n",
    "\n",
    "# let the query be...\n",
    "query = \"post office hours\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve weights for terms in sentence, + in query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokens[0] \n",
    "for w in sent:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = i_i\n",
    "query = \"what time is shark week\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params: self, query='', num_results=5, index=[], resources=[]\n",
    "'''\n",
    "Calculate relevance ranking\n",
    "query: String containing the query, this is a single string and it is not tokenized\n",
    "resources: Array of document id's that will be processed\n",
    "return: Array of ranked documents.\n",
    "'''\n",
    "\n",
    "ranker.relevance_ranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ranker.relevance_ranking(query = q,\n",
    "                           num_results=5,\n",
    "                            index=i_i.index,\n",
    "                            resources=resources,\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

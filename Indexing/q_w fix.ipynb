{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-db4d6e435bb1> INFO: 2019-10-12 17:03:25,676: Executing indexing module\n",
      "<ipython-input-5-db4d6e435bb1> INFO: 2019-10-12 17:03:25,676: Reading file\n",
      "persist_index_memory.py INFO: 2019-10-12 17:03:25,698: Instantiating storage object\n",
      "utilities.py INFO: 2019-10-12 17:03:25,698: Converting document to be ready to be indexed\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "inverted_index.py INFO: 2019-10-12 17:03:25,954: Instantiating inverted index object\n",
      "inverted_index.py DEBUG: 2019-10-12 17:03:25,955: Collection length = 12700\n",
      "12700it [00:39, 323.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: When is shark week?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 314/314 [00:00<00:00, 600005.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from classes.utilities import doc_utilities\n",
    "from classes.persist_index_memory import persist_index_memory\n",
    "from classes.inverted_index import inverted_index\n",
    "from classes.ranking import ranking\n",
    "from classes.preprocessing import preprocessing\n",
    "from classes.snip import snip\n",
    "from classes.match import match\n",
    "import nltk\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(filename)s %(levelname)s: %(asctime)s: %(message)s')\n",
    "logger = logging.getLogger('main')\n",
    "logger.info('Executing indexing module')\n",
    "logger.info('Reading file')\n",
    "\n",
    "# LOAD DATASET\n",
    "u = doc_utilities()\n",
    "u.read_data_set(file='data/12000_docs.p')\n",
    "memory_unit = persist_index_memory()\n",
    "u.process_documents_for_indexing()\n",
    "i_i = inverted_index(memory_unit)\n",
    "\n",
    "# CREATE INDEX FROM DATASET\n",
    "i_i.create_index(collection=u.get_collection_json(),\n",
    "                     process_text=True)\n",
    "i_i.create_term_document_matrix()\n",
    "\n",
    "# GIVEN QUERY FROM FRONT-END, FIND RELEVANT RESULTS\n",
    "query = 'When is shark week?' # user input\n",
    "print('input:',query)\n",
    "matcher = match()\n",
    "q = preprocessing().the_works(query)\n",
    "CR = i_i.lookup_query(q)\n",
    "CR = matcher.boolean(CR)\n",
    "# added in case not every token matches\n",
    "doctoken_matchnums =[len(i) for i in CR.values()]\n",
    "scaler = max(doctoken_matchnums)\n",
    "CR = matcher.scale(CR,scaler)\n",
    "\n",
    "# RANK RELEVANT RESULTS\n",
    "r_ranking = ranking()\n",
    "resources = list(CR.keys())\n",
    "max_freq = r_ranking.get_max_frequencies(index=CR) # , num_docs=len(i_i.storage.index)\n",
    "# Now save this into the persisted memory object within the index\n",
    "i_i.storage.max_frequency_terms_per_doc = max_freq\n",
    "res = r_ranking.relevance_ranking(query = query,\n",
    "                           num_results=5,\n",
    "                            index=i_i.index,\n",
    "                            resources=resources,\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE RANKED JSON_SNIPPETS FOR FRONT-END\n",
    "# snipper = snip(r_ranking)\n",
    "# json = snipper.get_snippets(res, resources=resources, query=query, i_i=i_i)\n",
    "print('output:',json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 120.96872540393481),\n",
       " (231, 120.96872540393481),\n",
       " (285, 120.96872540393481),\n",
       " (289, 120.96872540393481),\n",
       " (33, 60.484362701967406)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 377.69it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_split = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "json_frontend = []\n",
    "\n",
    "for id, weight in tqdm(res):\n",
    "    doc_id = resources[id]\n",
    "\n",
    "    # retrieve original unprocessed doc-string    \n",
    "    original = i_i.storage.get(doc_id)['content']\n",
    "\n",
    "    # get doc title\n",
    "    title, *text = original.split('\\n\\n')\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    # separate text into sentences\n",
    "    sentences = sent_split.tokenize(text)\n",
    "    # generate snippet\n",
    "#     doc_snippet = self.gen_snip(document=sentences, i_i=i_i, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"One Art\" is a poem by American poet Elizabeth Bishop.',\n",
       " 'The poem was included in her collection \"Geography III\".',\n",
       " \"It is also the name of a collection of Bishop's letters.\",\n",
       " 'Bishop wrote seventeen drafts of the poem, with titles including \"How to Lose Things,\" \"The Gift of Losing Things,\" and \"The Art of Losing Things\".',\n",
       " 'By the fifteenth draft, Bishop had chosen \"One Art\" as her title.',\n",
       " 'The poem was written over the course of two weeks, an unusually short time for Bishop.',\n",
       " 'Some of the piece is adapted from a longer poem, \"Elegy\", that Bishop never completed or published.',\n",
       " 'Bishop\\'s life was marked by loss and instability, which is reflected in many of the poems of \"Geography III\".',\n",
       " '\"One Art\" is narrated by a speaker who details losing small items, which gradually become more significant, moving, for example, from the misplacement of \"door keys\" to the loss of \"two cities\" where the speaker presumably lived.',\n",
       " 'The poem is a villanelle, an originally French poetic form known for generally dealing with pastoral themes.',\n",
       " 'Brad Leithauser wrote of the poem that, in addition to \"Do not go gentle into that good night\" by Dylan Thomas, that it \"...might have taken the elaborate stanzic arrangement even if the Italians hadn\\'t invented it three hundred years ago.\"',\n",
       " 'Brett Miller wrote that \"One Art\" \"may be the best modern example of a villanelle...\" along with Theodore Roethke\\'s \"The Waking\".']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_snip(document=[], i_i={}, query=''):\n",
    "    q = preproc.the_works(query)\n",
    "    q_max_freqs = r_ranking.get_max_frequencies(index=i_i.index, sentence_tokens=q)\n",
    "    i_i.storage.max_frequency_terms_per_doc = q_max_freqs\n",
    "    \n",
    "    print(q_max_freqs)\n",
    "        \n",
    "    # derive weights for tokens in query\n",
    "    q_weights=relevance_ranking(query = query,\n",
    "                           num_results=len(q),\n",
    "                            index=i_i.index,\n",
    "                            resources=[],\n",
    "                            max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "                            N=len(i_i.storage.index),\n",
    "                            term_doc_matrix=i_i.doc_term_matrix_all,\n",
    "                            weigh=True,\n",
    "                            qu_we=True)     \n",
    "    \n",
    "    q_w = [w[1] for w in sorted(q_weights, key=lambda x: x[0])]\n",
    "\n",
    "    print('num results for q_weights', query)\n",
    "    print('query weights generated', q_weights)\n",
    "    cos_score = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 10]\n",
      "num results for q_weights When is shark week?\n",
      "query weights generated [(0, 17.2812464862764), (1, 12.096872540393482)]\n"
     ]
    }
   ],
   "source": [
    "gen_snip(document=sentences, i_i=i_i, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_ranking(query='', num_results=5, index=[], resources=[], max_freq=[], N=0, term_doc_matrix=[],\n",
    "                      weigh=False, qu_we=False):\n",
    "    # Now we will preprocess and tokenize our query.\n",
    "    q=preproc.the_works(text=query)\n",
    "    results={}\n",
    "\n",
    "    if weigh: resources=q\n",
    "\n",
    "    # We will iterate through all the documents in the resources list\n",
    "    for id, val in enumerate(resources):\n",
    "        TF=0\n",
    "        IDF=0\n",
    "        #Iterate through query tokens\n",
    "#             print('Id = ', id, '-->', val)\n",
    "        for w in q:\n",
    "            if w not in index:\n",
    "                print('Term {} not found in index'.format(w))\n",
    "                continue\n",
    "            if not qu_we: freq = r_ranking.get_term_frequency(entries=index[w], doc_id=id)\n",
    "            else: freq = query.count(w) \n",
    "                \n",
    "            max_d = max_freq[id] #For base 0 reason\n",
    "            # Now calculate TF\n",
    "            if max_d==0: TF=0\n",
    "            else: TF=TF+freq/max_d\n",
    "            # Now calculate IDF\n",
    "            n_w=r_ranking.n_w(term_doc_matrix=term_doc_matrix, term=w)\n",
    "            IDF = IDF+math.log2(N/n_w)\n",
    "        # Now let's join the TF-IDF\n",
    "        results[id] = TF*IDF\n",
    "    sorted_result = sorted(results.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    return sorted_result[:num_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigh = True\n",
    "q=preproc.the_works(text=query)\n",
    "results={}\n",
    "\n",
    "if weigh: resources=q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'docId': 289, 'frequency': 2},\n",
       " {'docId': 2574, 'frequency': 1},\n",
       " {'docId': 3095, 'frequency': 1},\n",
       " {'docId': 3556, 'frequency': 7},\n",
       " {'docId': 3860, 'frequency': 1},\n",
       " {'docId': 5138, 'frequency': 1},\n",
       " {'docId': 7047, 'frequency': 1},\n",
       " {'docId': 7785, 'frequency': 1},\n",
       " {'docId': 9185, 'frequency': 4},\n",
       " {'docId': 9334, 'frequency': 1},\n",
       " {'docId': 9615, 'frequency': 1},\n",
       " {'docId': 9749, 'frequency': 1},\n",
       " {'docId': 10161, 'frequency': 2},\n",
       " {'docId': 10955, 'frequency': 1}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_i.index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word shark\n",
      "shark in index True\n",
      "entries [{'docId': 289, 'frequency': 2}, {'docId': 2574, 'frequency': 1}, {'docId': 3095, 'frequency': 1}, {'docId': 3556, 'frequency': 7}, {'docId': 3860, 'frequency': 1}, {'docId': 5138, 'frequency': 1}, {'docId': 7047, 'frequency': 1}, {'docId': 7785, 'frequency': 1}, {'docId': 9185, 'frequency': 4}, {'docId': 9334, 'frequency': 1}, {'docId': 9615, 'frequency': 1}, {'docId': 9749, 'frequency': 1}, {'docId': 10161, 'frequency': 2}, {'docId': 10955, 'frequency': 1}]\n",
      "freq 0\n",
      "TF 0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'term_doc_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-3599e799fb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Now calculate IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mn_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_doc_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterm_doc_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'term_doc_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# We will iterate through all the documents in the resources list\n",
    "for id, val in enumerate(resources):\n",
    "    TF=0\n",
    "    IDF=0\n",
    "    #Iterate through query tokens\n",
    "    for w in q:\n",
    "        print('word', w)\n",
    "        print(w, 'in index', w in i_i.index)\n",
    "        print('entries',i_i.index[w])\n",
    "        if w not in i_i.index:\n",
    "            print('Term {} not found in index'.format(w))\n",
    "            continue\n",
    "        if not qu_we: freq = r_ranking.get_term_frequency(entries=i_i.index[w], doc_id=id)\n",
    "        else: query.count(w)\n",
    "            \n",
    "        print('freq', freq)\n",
    "        \n",
    "        max_d = max_freq[id] #For base 0 reason\n",
    "        \n",
    "        # Now calculate TF\n",
    "        if max_d==0: TF=0\n",
    "        else: TF=TF+freq/max_d\n",
    "            \n",
    "        print('TF', TF)\n",
    "        \n",
    "        # Now calculate IDF\n",
    "        n_w=r_ranking.n_w(term_doc_matrix=term_doc_matrix, term=w)\n",
    "        IDF = IDF+math.log2(N/n_w)\n",
    "    \n",
    "        print('IDF', IDF)\n",
    "        \n",
    "    # Now let's join the TF-IDF\n",
    "    results[id] = TF*IDF\n",
    "sorted_result = sorted(results.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # save only the 2 closest sentences\n",
    "#     snippet = collections.deque(maxlen=2)\n",
    "\n",
    "#     # derive weights for sentences in document...\n",
    "#     for i in range(len(document)):\n",
    "#         s = document[i]\n",
    "#         s_max_freqs = r_ranking.get_max_frequencies(index=i_i.index, sentence_tokens=preproc.the_works(text=s))\n",
    "#         i_i.storage.max_frequency_terms_per_doc = s_max_freqs\n",
    "#         #print('num_results for s_weights',s)\n",
    "\n",
    "#         s_weights = r_ranking.relevance_ranking(query = s,\n",
    "#                                num_results=len(s),\n",
    "#                                 index=i_i.index,\n",
    "#                                 resources=[],\n",
    "#                                 max_freq=i_i.storage.max_frequency_terms_per_doc,\n",
    "#                                 N=len(i_i.storage.index),\n",
    "#                                 term_doc_matrix=i_i.doc_term_matrix_all,\n",
    "#                                 weigh=True)\n",
    "\n",
    "#         ordered_s_weights = [w[1] for w in sorted(s_weights, key=lambda x: x[0])]\n",
    "#         #print('weighted words',ordered_s_weights)\n",
    "\n",
    "#         #print('doc weights generated', ordered_s_weights)\n",
    "#         cosine_sim = cos_similarity(ordered_s_weights, q_w)\n",
    "\n",
    "#         print('cosine score', cosine_sim)\n",
    "\n",
    "#         #print('cosine similarity calculated', cosine_sim)\n",
    "#         if cosine_sim > cos_score:\n",
    "#             cos_score = cosine_sim\n",
    "#             snippet.append(s)\n",
    "#     return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "preproc = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import logging\n",
    "from classes.preprocessing import preprocessing\n",
    "import operator\n",
    "from classes.inverted_index import *\n",
    "from classes.ranking import ranking\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(d_weights, q_weights):\n",
    "    \"\"\"\n",
    "    calculate cosine similarity between 2 vectors (lists of numbers)\n",
    "    For snippet generation\n",
    "    \"\"\"\n",
    "    numerator = sum([d*q for d,q in zip(d_weights, q_weights)])\n",
    "    d_norm = sum([d*d for d in d_weights])\n",
    "    q_norm = sum([q*q for q in q_weights])\n",
    "    denom = np.sqrt(d_norm * q_norm)\n",
    "    return numerator/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets(ranked_results, resources=[], query=\"\", i_i={}):\n",
    "\n",
    "    #print('get_snippets called')\n",
    "    sent_split = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    json_frontend = []\n",
    "\n",
    "    for id, weight in tqdm(ranked_results):\n",
    "     #   print('id', id)\n",
    "        doc_id = resources[id]\n",
    "      #  print('doc id found')\n",
    "        # retrieve original unprocessed doc-string    \n",
    "        original = i_i.storage.get(doc_id)['content']\n",
    "       # print('doc retrieved from i_i\\'s stroage')\n",
    "        # get doc title\n",
    "        title, *text = original.split('\\n\\n')\n",
    "        text = ' '.join(text)\n",
    "\n",
    "        # separate text into sentences\n",
    "        sentences = sent_split.tokenize(text)\n",
    "\n",
    "     #   print('before gen snip call', id)\n",
    "        # generate snippet\n",
    "        doc_snippet = self.gen_snip(document=sentences, i_i=i_i, query=query)\n",
    "\n",
    "        # prepare for frontend\n",
    "        json_frontend.append({'title':title,     'snippet':'...'.join(doc_snippet)})\n",
    "\n",
    "      #  print('title', title, 'snippet','...'.join(doc_snippet))\n",
    "\n",
    "    return json_frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class snip:\n",
    "    \"\"\"\n",
    "    Class to handle generating snippets operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, ranker=''):\n",
    "        self.preproc = preprocessing()\n",
    "        self.ranker = ranker\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

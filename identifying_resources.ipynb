{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "From Gerardo's 'preprocessing.py'\n",
    "'''\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stemer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def stem(tokens=''):\n",
    "    return [stemer.stem(w) for w in tokens]\n",
    "def tokenize(text=''):\n",
    "    return tokenizer.tokenize(text)\n",
    "def remove_stopwords(tokens=[]):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stop_words]\n",
    "def remove_capitalization(tokens=[]):\n",
    "    return [w.lower() for w in tokens]\n",
    "def remove_punctuation(text=''):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npre-made sample inverted_index\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pre-made sample inverted_index\n",
    "'''\n",
    "# path = \"Documents/Classes/CS537_IR/project1/\"\n",
    "# with open('inverted_index_chunk1.pkl', 'rb') as handle:\n",
    "#     b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process query **q** using same techniques as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"what time is shark week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text=''):\n",
    "    # Remove punctuation from the text.\n",
    "    text = remove_punctuation(text=text)\n",
    "    # Tokenize\n",
    "    tokens = tokenize(text=text)\n",
    "    # Remove stop words\n",
    "    tokens = remove_stopwords(tokens=tokens)\n",
    "    # Remove capitalization\n",
    "    tokens = remove_capitalization(tokens=tokens)\n",
    "    # Stem terms\n",
    "    tokens = stem(tokens=tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'shark', 'week']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = preprocess(q)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create set of docs [**CR**] comprised of all docs in Document Collection [**DC**] that contains ALL of the terms in the query [**q**]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my BS\n",
    "prefix = \"~/Documents/Classes/CS537_IR/project1/chunks/\"\n",
    "sample_path = prefix + 'chunk_1.pkl' \n",
    "doc_collection = pd.read_pickle(sample_path)\n",
    "\n",
    "# from Gerardo\n",
    "json_collection = doc_collection.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = json_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn DC into our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "from Gerardo's class inverted_index\n",
    "'''\n",
    "class inverted_index:\n",
    "    \n",
    "    def __init__(self, storage = ''):\n",
    "        self.index = {}\n",
    "#         self.storage = storage\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        String representation of the Database object\n",
    "        \"\"\"\n",
    "        return str(self.index)\n",
    "\n",
    "    def index_document(self, document=''):\n",
    "        \"\"\"\n",
    "        Process a given document, save it to the DB and update the index.\n",
    "        \"\"\"\n",
    "        tokens = preprocess(document['content'])\n",
    "\n",
    "        appearances_dict = dict()\n",
    "        # Dictionary with each term and the frequency it appears in the text.\n",
    "        for term in tokens:\n",
    "            term_frequency = appearances_dict[term].frequency if term in appearances_dict else 0\n",
    "            appearances_dict[term] = Appearance(document['id'], term_frequency + 1)\n",
    "        # Update the inverted index\n",
    "        update_dict = {key: [appearance]\n",
    "        if key not in self.index\n",
    "        else self.index[key] + [appearance]\n",
    "                       for (key, appearance) in appearances_dict.items()}\n",
    "        self.index.update(update_dict)\n",
    "        # Add the document into the database\n",
    "#         self.storage.add(document)\n",
    "\n",
    "        return document\n",
    "\n",
    "    def lookup_query(self, query):\n",
    "        \"\"\"\n",
    "        Returns the dictionary of terms with their correspondent Appearances.\n",
    "        This is a very naive search since it will just split the terms and show\n",
    "        the documents where they appear.\n",
    "        \"\"\"   \n",
    "        return {term: self.index[term] for term in query if term in self.index}\n",
    "\n",
    "    def create_index(self, collection=[]):\n",
    "        for i, doc in tqdm(enumerate(collection)):\n",
    "            self.index_document(document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Appearance:\n",
    "    \"\"\"\n",
    "    Represents the appearance of a term in a given document, along with the\n",
    "    frequency of appearances in the same one.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, docId, frequency):\n",
    "        self.docId = docId\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        String representation of the Appearance object\n",
    "        \"\"\"\n",
    "        return str(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrunning this on a 100,000 doc chunk of the dataset\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "running this on a 100,000 doc chunk of the dataset\n",
    "'''\n",
    "# i_i = inverted_index()\n",
    "# i_i.create_index(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reminder: create set of docs [**CR**] comprised of all docs in Document Collection [**DC**] that contains ALL of the terms in the query [**q**]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = i_i.lookup_query(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_matches(CR={}, len_DC=int):\n",
    "    doc_first = {}\n",
    "    for i in range(1, len_DC+1):\n",
    "        doc_first[i] = []    \n",
    "\n",
    "    for token in CR:\n",
    "        for val in CR[token]:\n",
    "            match = ('match: ' + token, 'freq: ' + str(val.frequency))\n",
    "            doc_first[val.docId].append(match)\n",
    "\n",
    "    return {k: doc_first[k] for k in doc_first if len(doc_first[k]) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = return_matches(srtd, len(dc))\n",
    "len(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_scaling(CR = {}, match_num = int):\n",
    "    return {k: CR[k] for k in CR if len(CR[k]) == match_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_scaling(CR, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If **CR > 50**, consider the docs in the Document Collection with **n-1** terms in the query (\"all the combinations\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_scaling(CR, 2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(match_scaling(CR, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
